{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNY3BAXi0i2T+trRY0g+RBd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AyushBhandari03/NLP/blob/main/nlp_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KT6eGpwbsKj9"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwSUHZOqtmdJ",
        "outputId": "0651ca8e-e3f3-4b82-ddc5-b7be277ed881"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca334984"
      },
      "source": [
        "corpus =\"This is a sample sentence that we will use to demonstrate the process of tokenization and stemming. We will use the NLTK library to perform these operations. NLTK is a powerful library for natural language processing tasks. It provides a wide range of tools and resources for working with text data. We can use NLTK to perform tasks such as tokenization, stemming, lemmatization, part-of-speech tagging, and named entity recognition. In this example, we will focus on tokenization and stemming. Tokenization is the process of breaking down a text into individual words or tokens. Stemming is the process of reducing words to their root form. For example, the words 'running', 'runs', and 'ran' can all be reduced to the stem 'run'. Lemmatization is similar to stemming, but it reduces words to their base or dictionary form, called the lemma. For example, the lemma of 'running' is 'run'. Stemming is generally faster than lemmatization, but lemmatization is more accurate. We can use NLTK to perform both stemming and lemmatization. We will also demonstrate how to remove stop words from the text. Stop words are common words that do not carry much meaning, such as 'the', 'a', and 'is'. Removing stop words can help to improve the performance of natural language processing models. We will use the NLTK corpus of stop words to remove stop words from the text. Finally, we will print the tokenized and stemmed text. This will show us how the text has been processed. We can then use the processed text for further analysis or modeling.\""
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "uBethgbYu2f5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lowercasetoken=[tok.lower() for tok in tokens]\n",
        "print(lowercasetoken)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B76yIBMYve0H",
        "outputId": "1876c8e0-74d2-46fa-8b67-938d3088d7b5"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'sample', 'sentence', 'that', 'we', 'will', 'use', 'to', 'demonstrate', 'the', 'process', 'of', 'tokenization', 'and', 'stemming', '.', 'we', 'will', 'use', 'the', 'nltk', 'library', 'to', 'perform', 'these', 'operations', '.', 'nltk', 'is', 'a', 'powerful', 'library', 'for', 'natural', 'language', 'processing', 'tasks', '.', 'it', 'provides', 'a', 'wide', 'range', 'of', 'tools', 'and', 'resources', 'for', 'working', 'with', 'text', 'data', '.', 'we', 'can', 'use', 'nltk', 'to', 'perform', 'tasks', 'such', 'as', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'and', 'named', 'entity', 'recognition', '.', 'in', 'this', 'example', ',', 'we', 'will', 'focus', 'on', 'tokenization', 'and', 'stemming', '.', 'tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.', 'stemming', 'is', 'the', 'process', 'of', 'reducing', 'words', 'to', 'their', 'root', 'form', '.', 'for', 'example', ',', 'the', 'words', \"'running\", \"'\", ',', \"'runs\", \"'\", ',', 'and', \"'ran\", \"'\", 'can', 'all', 'be', 'reduced', 'to', 'the', 'stem', \"'run\", \"'\", '.', 'lemmatization', 'is', 'similar', 'to', 'stemming', ',', 'but', 'it', 'reduces', 'words', 'to', 'their', 'base', 'or', 'dictionary', 'form', ',', 'called', 'the', 'lemma', '.', 'for', 'example', ',', 'the', 'lemma', 'of', \"'running\", \"'\", 'is', \"'run\", \"'\", '.', 'stemming', 'is', 'generally', 'faster', 'than', 'lemmatization', ',', 'but', 'lemmatization', 'is', 'more', 'accurate', '.', 'we', 'can', 'use', 'nltk', 'to', 'perform', 'both', 'stemming', 'and', 'lemmatization', '.', 'we', 'will', 'also', 'demonstrate', 'how', 'to', 'remove', 'stop', 'words', 'from', 'the', 'text', '.', 'stop', 'words', 'are', 'common', 'words', 'that', 'do', 'not', 'carry', 'much', 'meaning', ',', 'such', 'as', \"'the\", \"'\", ',', \"'\", 'a', \"'\", ',', 'and', \"'is\", \"'\", '.', 'removing', 'stop', 'words', 'can', 'help', 'to', 'improve', 'the', 'performance', 'of', 'natural', 'language', 'processing', 'models', '.', 'we', 'will', 'use', 'the', 'nltk', 'corpus', 'of', 'stop', 'words', 'to', 'remove', 'stop', 'words', 'from', 'the', 'text', '.', 'finally', ',', 'we', 'will', 'print', 'the', 'tokenized', 'and', 'stemmed', 'text', '.', 'this', 'will', 'show', 'us', 'how', 'the', 'text', 'has', 'been', 'processed', '.', 'we', 'can', 'then', 'use', 'the', 'processed', 'text', 'for', 'further', 'analysis', 'or', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "swtokens=[tok for tok in lowercasetoken if tok not in stopwords.words('english')]"
      ],
      "metadata": {
        "id": "_WtTpVFnwbtn"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(swtokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1vpCsVXxFF4",
        "outputId": "3e42721f-ee58-4b29-cbf1-7f45c263ace1"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', 'use', 'demonstrate', 'process', 'tokenization', 'stemming', '.', 'use', 'nltk', 'library', 'perform', 'operations', '.', 'nltk', 'powerful', 'library', 'natural', 'language', 'processing', 'tasks', '.', 'provides', 'wide', 'range', 'tools', 'resources', 'working', 'text', 'data', '.', 'use', 'nltk', 'perform', 'tasks', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'named', 'entity', 'recognition', '.', 'example', ',', 'focus', 'tokenization', 'stemming', '.', 'tokenization', 'process', 'breaking', 'text', 'individual', 'words', 'tokens', '.', 'stemming', 'process', 'reducing', 'words', 'root', 'form', '.', 'example', ',', 'words', \"'running\", \"'\", ',', \"'runs\", \"'\", ',', \"'ran\", \"'\", 'reduced', 'stem', \"'run\", \"'\", '.', 'lemmatization', 'similar', 'stemming', ',', 'reduces', 'words', 'base', 'dictionary', 'form', ',', 'called', 'lemma', '.', 'example', ',', 'lemma', \"'running\", \"'\", \"'run\", \"'\", '.', 'stemming', 'generally', 'faster', 'lemmatization', ',', 'lemmatization', 'accurate', '.', 'use', 'nltk', 'perform', 'stemming', 'lemmatization', '.', 'also', 'demonstrate', 'remove', 'stop', 'words', 'text', '.', 'stop', 'words', 'common', 'words', 'carry', 'much', 'meaning', ',', \"'the\", \"'\", ',', \"'\", \"'\", ',', \"'is\", \"'\", '.', 'removing', 'stop', 'words', 'help', 'improve', 'performance', 'natural', 'language', 'processing', 'models', '.', 'use', 'nltk', 'corpus', 'stop', 'words', 'remove', 'stop', 'words', 'text', '.', 'finally', ',', 'print', 'tokenized', 'stemmed', 'text', '.', 'show', 'us', 'text', 'processed', '.', 'use', 'processed', 'text', 'analysis', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer=WordNetLemmatizer()\n",
        "filtered_tok=[lemmatizer.lemmatize(tok) for tok in swtokens]"
      ],
      "metadata": {
        "id": "TP3LJxXXxzWg"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(filtered_tok)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IbvOVEQxeLz",
        "outputId": "e7787427-eb73-4a2f-9218-cc38ceefa4ba"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sample', 'sentence', 'use', 'demonstrate', 'process', 'tokenization', 'stemming', '.', 'use', 'nltk', 'library', 'perform', 'operation', '.', 'nltk', 'powerful', 'library', 'natural', 'language', 'processing', 'task', '.', 'provides', 'wide', 'range', 'tool', 'resource', 'working', 'text', 'data', '.', 'use', 'nltk', 'perform', 'task', 'tokenization', ',', 'stemming', ',', 'lemmatization', ',', 'part-of-speech', 'tagging', ',', 'named', 'entity', 'recognition', '.', 'example', ',', 'focus', 'tokenization', 'stemming', '.', 'tokenization', 'process', 'breaking', 'text', 'individual', 'word', 'token', '.', 'stemming', 'process', 'reducing', 'word', 'root', 'form', '.', 'example', ',', 'word', \"'running\", \"'\", ',', \"'runs\", \"'\", ',', \"'ran\", \"'\", 'reduced', 'stem', \"'run\", \"'\", '.', 'lemmatization', 'similar', 'stemming', ',', 'reduces', 'word', 'base', 'dictionary', 'form', ',', 'called', 'lemma', '.', 'example', ',', 'lemma', \"'running\", \"'\", \"'run\", \"'\", '.', 'stemming', 'generally', 'faster', 'lemmatization', ',', 'lemmatization', 'accurate', '.', 'use', 'nltk', 'perform', 'stemming', 'lemmatization', '.', 'also', 'demonstrate', 'remove', 'stop', 'word', 'text', '.', 'stop', 'word', 'common', 'word', 'carry', 'much', 'meaning', ',', \"'the\", \"'\", ',', \"'\", \"'\", ',', \"'is\", \"'\", '.', 'removing', 'stop', 'word', 'help', 'improve', 'performance', 'natural', 'language', 'processing', 'model', '.', 'use', 'nltk', 'corpus', 'stop', 'word', 'remove', 'stop', 'word', 'text', '.', 'finally', ',', 'print', 'tokenized', 'stemmed', 'text', '.', 'show', 'u', 'text', 'processed', '.', 'use', 'processed', 'text', 'analysis', 'modeling', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d4c1f52",
        "outputId": "1047d69b-6f5e-43b8-87b5-944f3cad525c"
      },
      "source": [
        "from nltk import FreqDist\n",
        "\n",
        "fdist = FreqDist(filtered_tok)\n",
        "print(\"Frequency of each element:\")\n",
        "for word, frequency in fdist.most_common(50):\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frequency of each element:\n",
            ".: 20\n",
            ",: 16\n",
            "word: 10\n",
            "': 10\n",
            "stemming: 7\n",
            "text: 7\n",
            "use: 6\n",
            "nltk: 5\n",
            "lemmatization: 5\n",
            "stop: 5\n",
            "tokenization: 4\n",
            "process: 3\n",
            "perform: 3\n",
            "example: 3\n",
            "demonstrate: 2\n",
            "library: 2\n",
            "natural: 2\n",
            "language: 2\n",
            "processing: 2\n",
            "task: 2\n",
            "form: 2\n",
            "'running: 2\n",
            "'run: 2\n",
            "lemma: 2\n",
            "remove: 2\n",
            "processed: 2\n",
            "sample: 1\n",
            "sentence: 1\n",
            "operation: 1\n",
            "powerful: 1\n",
            "provides: 1\n",
            "wide: 1\n",
            "range: 1\n",
            "tool: 1\n",
            "resource: 1\n",
            "working: 1\n",
            "data: 1\n",
            "part-of-speech: 1\n",
            "tagging: 1\n",
            "named: 1\n",
            "entity: 1\n",
            "recognition: 1\n",
            "focus: 1\n",
            "breaking: 1\n",
            "individual: 1\n",
            "token: 1\n",
            "reducing: 1\n",
            "root: 1\n",
            "'runs: 1\n",
            "'ran: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IkGTqMUozh4H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}